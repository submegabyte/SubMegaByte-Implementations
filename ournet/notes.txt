Idea
View guided Mamba-Transformer point cloud completion
View guided block-state point cloud completion

pointnet, vipc only use point and global features

pointmae, pointmamba, pointramba extend pointnet by incorporating
local patch features. we can do the same for vipc

3dmambacomplete doesn't utilize an image view, instead creates
hyperpoints and disperses them, which makes the model rigid

mamba ordering/sequence/scan
we can use bio from pointramba

pointmae's robustness to missing patches

vipc's ability to fill missing information from an image reference

large data efficiency from mamba

self-attention from vision transfomer (used in pointmae, )

cross-attention from block-state transformer

---------------

transformer + bio for mamba input ordering

mamba for processing all the patches together from image and pointcloud
basically add context across all input

self attention b/w patches of the pointcloud for robustness (from pointmae)
cross attention from image to pointcloud to fill missing info

----------------

mamba was introduced as an quicker alternative to self attention,
no strong cross attention references yet
https://github.com/state-spaces/mamba/issues/229

mamba for long input encoding to learn contextual cues

then use block-state transformer for shorter input chunks
to get cross attention and short chunk self attention

types of cross attention
1. two point clouds
2. one point cloud, one image

----------------

generate input embeddings

ordering

mamba context

self attention
cross attention

decode

----------------

image
b c h w

image patch
b c hp wp

patches per image
h/ph * w/pw 

patch embedding
b e

patch encoder:
self-attention vit blocks

----------------

pointcloud
b n 3

pointcloud patch (KNN)
b k 3

patch embedding
b e

-----------------

sfe and sftnet

vit was used in egiinet
can replace with block-state transformer

don't forget mae strengthening